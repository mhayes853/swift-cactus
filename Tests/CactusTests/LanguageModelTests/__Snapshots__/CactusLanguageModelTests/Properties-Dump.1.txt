▿ Properties
  - attentionHeadDimensions: 64
  - attentionHeads: 32
  - attentionKVHeads: 8
  - contextLengthTokens: 128000
  - expertCount: 0
  - ffnIntermediateDimensions: 12288
  - hiddenDimensions: 2048
  - layerCount: 16
  - layerNormEpsilon: 1e-05
  ▿ modelType: ModelType
    - defaultTemperature: 0.3
    - defaultTopK: 20
    - defaultTopP: 0.95
    - identifier: "lfm2"
  - moeEveryNLayers: 0
  ▿ precision: Precision
    - bits: 16
    - isFloatingPoint: true
  - ropeTheta: 1000000.0
  - sharedExpertCount: 0
  - shouldTieWordEmbeddings: true
  - topExpertCount: 0
  - vocabularySize: 65536
